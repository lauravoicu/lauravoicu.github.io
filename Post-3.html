<!DOCTYPE html>
<html style="font-size: 16px;">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content="">
    <meta name="description" content="">
    <meta name="page_type" content="np-template-header-footer-from-plugin">
    <title>Post 3</title>
    <link rel="stylesheet" href="nicepage.css" media="screen">
<link rel="stylesheet" href="Post-3.css" media="screen">
    <script class="u-script" type="text/javascript" src="jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="nicepage.js" defer=""></script>
    <meta name="generator" content="Nicepage 3.16.1, nicepage.com">
    <link id="u-theme-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i|Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i">
    <link id="u-page-google-font" rel="stylesheet" href="https://fonts.googleapis.com/css?family=Oswald:200,300,400,500,600,700">
    
    
    
    
    
    
    
    
    
    
    
    
    <script type="application/ld+json">{
		"@context": "http://schema.org",
		"@type": "Organization",
		"name": "Laura Voicu",
		"url": "index.html"
}</script>
    <meta property="og:title" content="Post 3">
    <meta property="og:type" content="website">
    <meta name="theme-color" content="#40bcdf">
    <link rel="canonical" href="index.html">
    <meta property="og:url" content="index.html">
  </head>
  <body class="u-body u-overlap"><header class="u-align-left u-black u-clearfix u-header u-header" id="sec-bc4c"><div class="u-clearfix u-sheet u-sheet-1"></div></header>
    <section class="u-clearfix u-section-1" id="sec-8305">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <div class="u-clearfix u-expanded-width u-layout-wrap u-layout-wrap-1">
          <div class="u-layout">
            <div class="u-layout-row">
              <div class="u-container-style u-layout-cell u-left-cell u-size-30 u-layout-cell-1">
                <div class="u-container-layout u-valign-middle u-container-layout-1">
                  <h2 class="u-custom-font u-font-oswald u-text u-text-1">How to Build an Image Classifier with Very Little Data (Part 2)<br>
                  </h2>
                  <p class="u-text u-text-2">Hi! My name is Laura, and this is my blog. My opinions are my own, and you are free to challenge them, like them or share them.</p>
                </div>
              </div>
              <div class="u-container-style u-image u-layout-cell u-right-cell u-size-30 u-image-1" data-image-width="1280" data-image-height="853">
                <div class="u-container-layout u-container-layout-2"></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="u-align-center u-clearfix u-section-2" id="sec-cc97">
      <div class="u-clearfix u-sheet u-sheet-1">
        <h3 class="u-align-left u-text u-text-1">How to Build an Image Classifier with Very Little Data (Part 2)</h3>
        <p class="u-align-left u-text u-text-2">In the first part of this article I presented the results of the classical machine learning classifiers and introduced the convolutional neural network architecture. In this second part, let's take a look at different strategies to improve the accuracy of the model.</p>
      </div>
    </section>
    <section class="u-clearfix u-section-3" id="sec-cced">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-align-left u-text u-text-1">Before we go into the different techniques I tried to improve the acurracy, let's take a step back and go through a few basics.<br>
          <br>While training a convolutional neural network myself, it can become overwhelming to keep track of all the number of decisions that need to be made and the what seems like almost infinite
possibilities to be explored. This is where pipelines come in handy to experiment with different scenarios. Let's take a quick look at the exploratory data analysis which inspired the pipeline plan.&nbsp;<br>
        </p>
        <img src="images/eda.jpg" alt="" class="u-image u-image-default u-image-1" data-image-width="814" data-image-height="614">
        <p class="u-align-left u-text u-text-2">Analysis: <br>• The
images are in different sizes <br>• The
image brightness is fairly random <br>• The
images may be slightly rotated <br>• The
images may not be facing straight <br>• The
images may not be exactly centred <br>• The
class are fairly equally distributed (most classes ~15% of overall dataset, one class 20% of overall dataset) and
consistent across training, validation and test data set
        </p>
        <p class="u-text u-text-3">Visualization helps me to intuitively understand what I’m dealing with. Some ideas I explored here are: resize all images into the same shape, image augmentation to compensate for few training data samples, data normalization, experimentation with different color spaces&nbsp;</p>
      </div>
    </section>
    <section class="u-align-left u-clearfix u-section-4" id="sec-2bc9">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <div class="fr-view u-clearfix u-rich-text u-text u-text-1">
          <h2 style="text-align: left;">Data Normalization</h2>
          <p>There are three main types of pixel scaling techniques supported by the Keras ImageDataGenerator class:</p>
          <p>•Pixel Normalization: scale pixel values to the range 0-1.</p>
          <p>•Pixel Centering: scale pixel values to have a zero mean.</p>
          <p>•Pixel Standardization: scale pixel values to have a zero mean and unit variance.</p>
          <p>The pixel standardization is supported at two levels: either per-image (called sample-wise) or per-dataset (called feature-wise). Specifically, the mean and/or mean and standard deviation statistics required to standardize pixel values can be calculated from the pixel values in each image only (sample-wise) or across the entire training dataset (feature-wise).</p>
          <p style="text-align: left;">
            <span style="line-height: 2.0;">
              <img align="center" style="width: 722px;" class="fr-dii fr-fic fr-fil" src="images/norm.jpg" width="570">
            </span>
          </p>
          <p>Feature-wise normalization (left) implies the following:</p>
          <p>- featurewise_center: set input mean to 0 over the dataset.</p>
          <p>- featurewise_std_normalization: divide inputs by std of the dataset.</p>
          <p>The results are a bit disappointing. I was expecting normalization to have a big effect, but the improvement is minimal.</p>
          <p>Sample-wise normalization (right) implies the following:</p>
          <p>- samplewise_center: set each sample mean to 0.</p>
          <p>- samplewise_std_normalization: divide each input by its std.</p>
          <p>Sample-wise normalization shows a bit better score (but higher loss) than the feature-wise normalization, but same as the feature-wise normalization not a significant improvement. And the model is still overfitting.</p>
        </div>
      </div>
    </section>
    <section class="u-align-left u-clearfix u-section-5" id="carousel_237e">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <div class="fr-view u-clearfix u-rich-text u-text u-text-1">
          <h2 style="text-align: left;">Data Augmentation</h2>
          <p style="text-align: left;">
            <span style="line-height: 2.0;">One missing ingredient required in modern convolutional neural networks is data augmentation. The human vision system is excellent at adapting to image translations, rotations and other forms of distortions. Take an image and flip it anyway, most people can still recognize it. However, CNNs are not very good at handling such distortions, they could fail terribly due to minor translations. They key to resolving this is to randomly distort the training images, using horizontal flipping, vertical flipping, rotation, zooming, shifting and other distortions. This would enable CNNs to learn how to handle these distortions, hence, they would be able to work well in the real world. For small datasets, data augmentation is used as a way to introduce variance and provide the classifier with a better sense of the world. &nbsp;</span>
          </p>
          <h2 style="text-align: left;">
            <img align="center" style="width: 1133px;" class="fr-dii fr-fic fr-fil" src="images/augm.jpg" width="570">
          </h2>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>
            <br>
          </p>
          <p>The performance with the augmentation is actually worse than without it. Moreover:</p>
          <p>• the network is not robust to these changes.</p>
          <p>• the training requires more epochs (it takes more time to train with larger data, and the network does not converge even after 500 epochs).&nbsp;</p>
          <p>
            <img src="images/augm_res.jpg" width="570" align="center" style="width: 570px;" class="fr-dii fr-fic fr-fil">
            <br>
          </p>
          <p>Still, I was not ready to abandon this path just yet.&nbsp;</p>
        </div>
      </div>
    </section>
    <section class="u-align-left u-clearfix u-section-6" id="sec-95be">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <div class="fr-view u-clearfix u-rich-text u-text u-text-1">
          <h2 style="text-align: left;">Color Space Transformations</h2>
          <p style="text-align: left;">Being able to handle distortion is a nice feature, but it isn't enough since our augmented samples are still highly correlated. So, I’ve used additional transformations like converting the color of an image from one color space to another to supplement our data while adding healthy variance.</p>
          <p style="text-align: left;">
            <span style="line-height: 2.0;">&nbsp;<img align="center" style="width: 573px;" class="fr-dii fr-fic fr-fil" src="images/cspace.jpg" width="570">
            </span>I've enjoyed this part very much, especially since it was the first time I've used color space transformations. After some research, I decided to use BGR, HSV, YUV, CrCb, HLS, Lab, Luv, XYZ.
          </p>
          <p>My aim here was not to create a very high-tech state of the art CNN architecture to get the best accuracy but to compare the color spaces.</p>
          <p>
            <br>
          </p>
          <p>The results were surprising. I was expecting some color transformations to be more efficient as many images are more about shapes and less about colors. Also, I was thinking that the colors in cars are more saturated than that of backgrounds (i.e., trees), and the color space like HSV and HLS might contribute to superior performance. This was not the case.<img src="images/cspace_res.jpg" width="570" align="center" style="width: 570px;" class="fr-dii fr-fic fr-fir">
          </p>
          <p>The results are generally quite similar. XYZ colorspace gives slightly better results while HSV gives slightly poorer results. Why did this happen? Maybe because XYZ is pretty similar to RGB, whereas HSV is a cylindrical system and is the farthest off RGB in terms of similarity, hence gave the worst results.</p>
        </div>
      </div>
    </section>
    <section class="u-clearfix u-section-7" id="sec-a01c">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-1">So far I’ve explored the hypothesis that by augmenting our
dataset, we allow our model to learn more robust and generalizable features and
produce more accurate classifications than our previous model.&nbsp;But there
are also ways to improve the performance of our model by fine-tuning the model
itself.&nbsp;Tuning hyperparameters for deep neural network can
be difficult as it can be slow to train a deep neural network and there are
numerous parameters to configure. The most commonly used recipes are:<br>
          <br>-&nbsp;<b>Dropout </b>is a simple technique that will randomly drop nodes
out of the network, forcing the remaining nodes to adapt and pick-up the slack
of the removed nodes. In a way, it prevents a layer from seeing twice the exact
same pattern.<br>- During training, there will be a point when the model will
stop generalizing and start learning the statistical noise in the training dataset.&nbsp; <b>Early stopping </b>refers
stopping the training process before the learner passes that point.<br>-&nbsp;<b>Weight
Decay </b>tries to incentivize the network to use smaller
weights by adding a penalty to the loss function.<br>
          <br>But the list goes on and on (and this is by no means an exhaustive list): the number
of convolutional layers and filters, the number
of fully connected (dense) layers and neurons, the number
of epochs, the batch
size, the learning
rate, using dropout
and increasing dropout, batch
normalization, different
activations, different
optimizers and much more.<br>
          <br>As I experimented with more and more ideas, it became harder and harder
for me to remember what I had tried.
						What changes made the network better or worse? I was inspired by this post to use a mind map to keep track of the important things I tried.&nbsp;<br>
          <br>
        </p>
        <img src="images/mindmap.jpg" alt="" class="u-image u-image-default u-image-1" data-image-width="1600" data-image-height="910">
      </div>
    </section>
    <section class="u-clearfix u-section-8" id="sec-e4e9">
      <div class="u-clearfix u-sheet u-sheet-1">
        <p class="u-text u-text-1">I probably tried more than 15 different network architectures, with different network depths and different filter sizes, and learning rates. I got the the best results by using a very high filter size 11x11 (probably because since the pictures are mostly well centered, a big number of pixels is necessary
for the network recognize the object) and a very low learning rate. After 1000 epochs and 25 hours later, I decided to stop the training. It seems to the right path, but the network could still learn and could take advantage of more training to reach convergence.<br>
          <br>
        </p>
        <img src="images/res.jpg" alt="" class="u-image u-image-default u-image-1" data-image-width="858" data-image-height="352">
      </div>
    </section>
    <section class="u-clearfix u-section-9" id="sec-df60">
      <div class="u-clearfix u-sheet u-valign-middle u-sheet-1">
        <div class="u-expanded-width u-table u-table-responsive u-table-1">
          <table class="u-table-entity u-table-entity-1">
            <colgroup>
              <col width="33.3%">
              <col width="33.3%">
              <col width="33.400000000000006%">
            </colgroup>
            <thead class="u-table-header u-table-header-1">
              <tr style="height: 22px;">
                <th class="u-border-2 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Classifier</th>
                <th class="u-border-2 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Optimization</th>
                <th class="u-border-2 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Acurracy</th>
              </tr>
            </thead>
            <tbody class="u-table-body">
              <tr style="height: 75px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Dummy
classifier (most-frequent)</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">None</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.207</td>
              </tr>
              <tr style="height: 75px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Logistic Regression</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Grid search for optimal regularization strength</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.878</td>
              </tr>
              <tr style="height: 75px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">k-Nearest Neighbors</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Grid search for optimal k (number of neighbors), weights, distance types</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.880</td>
              </tr>
              <tr style="height: 76px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Linear SVM</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">PCA to reduce the number of features and retain 90+% of the variance&nbsp;<br>Grid search for optimal regularization parameter
                </td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.863</td>
              </tr>
              <tr style="height: 76px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">(Simple) Decision Trees</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Grid search for optimal tree depth (minimal)</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.840</td>
              </tr>
              <tr style="height: 76px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Random Forests</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Randomized search for optimal number of estimators (minimal)</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.880</td>
              </tr>
              <tr style="height: 76px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Convolutional Neural Networks</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Baseline + Data Augmentation +
  Batch Normalization + Dropout (0.1/0.5) + Filter size (11x11) + Number of
  filters (16+)&nbsp;</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.726</td>
              </tr>
              <tr style="height: 76px;">
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Convolutional Neural Networks</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">Baseline + Color Space Transformation (XYZ) + Dropout (0.3/0.5) + Filter size (3x3) +
Number of filters (32+)</td>
                <td class="u-border-1 u-border-grey-40 u-border-no-left u-border-no-right u-table-cell">0.597</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </section>
    <section class="u-align-left u-clearfix u-section-10" id="sec-7558">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="fr-view u-clearfix u-rich-text u-text u-text-1">
          <h2 style="text-align: left;">
            <span style="line-height: 2.0;">Reflections</span>
          </h2>
          <ul>
            <li>Data is very important. While researching on the topic I came across many articles/papers claiming to have solutions for the problem of having very little data. On a closer look that was defined there as very little data was typically between 500 and 1000 samples per class. Having only 50 samples per class, this project was extremely challenging.</li>
            <li>Transfer learning helped traditional ML models to compensate for the lack of data (even with minimal fine-tuning as was the case for Random Forests) and still outperform the convolutional neural network.</li>
            <li>The Mind Map as a tool to find information quickly and get some inspiration (put the experiments in a map around the most successful path).</li>
            <li>Time</li>
          </ul>
          <h2 style="text-align: left;">Ideas</h2>
          <ul>
            <li>Different data augmentation (generative model)</li>
            <li>Regularization techniques (L2 to force small parameters, L1 to set small parameters to 0), different activation/optimizer, filter sizes, learning rate reduction on plateau, color space transformation…)</li>
            <li>Retrain on wrongly predicted training images (fine tune the model to specific set of images for which it previously miss predicted).</li>
            <li>Ensembles or make a gradient boosted tree of neural networks.</li>
          </ul>
        </div>
      </div>
    </section>
    <section class="u-align-left u-clearfix u-section-11" id="sec-90d3">
      <div class="u-clearfix u-sheet u-sheet-1">
        <div class="fr-view u-clearfix u-rich-text u-text u-text-1">
          <h2 style="text-align: left;">References and interesting reads:</h2>
          <ul>
            <li style="text-align: left;">
              <span style="line-height: 2.0;">"The Effectiveness of Data Augmentation in Image Classification using Deep Learning", Perez et al., 2017.</span>
            </li>
            <li style="text-align: left;">"Why do deep convolutional networks generalize so poorly to small image transformations?", Azulay et al., 2019</li>
            <li>"Evolution of Convolutional Neural Network Architecture in Image Classification Problems", Arsenov et al. &nbsp; &nbsp; &nbsp;</li>
            <li style="box-sizing: border-box; -webkit-user-drag: element !important; user-select: text !important; outline: 0px !important; cursor: text !important; pointer-events: auto !important; text-align: left;">Data Augmentation: &nbsp;https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/</li>
            <li style="box-sizing: border-box; -webkit-user-drag: element !important; user-select: text !important; outline: 0px !important; cursor: text !important; pointer-events: auto !important; text-align: left;">Keras Data Augmentation:</li>
            <li>Pipeline and mindmap:</li>
            <li style="text-align: left;">
              <span style="line-height: 2.0;">My Github:</span>
            </li>
          </ul>
        </div>
      </div>
    </section>
    
    
    <footer class="u-align-left u-clearfix u-footer u-grey-80 u-footer" id="sec-1b5c"><div class="u-clearfix u-sheet u-sheet-1"></div></footer>
    <section class="u-backlink u-clearfix u-grey-80">
      <a class="u-link" href="https://nicepage.com/website-templates" target="_blank">
        <span>Web Templates</span>
      </a>
      <p class="u-text">
        <span>created with</span>
      </p>
      <a class="u-link" href="https://nicepage.com/static-site-generator" target="_blank">
        <span>Static Site Generator</span>
      </a>. 
    </section>
  </body>
</html>